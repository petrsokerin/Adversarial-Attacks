{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "from ignite.handlers.param_scheduler import create_lr_scheduler_with_warmup\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.metrics import mean_absolute_percentage_error as MAPE\n",
    "\n",
    "from typing import Dict, Any, Tuple, List, Union, Sequence, Callable\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'FordA'\n",
    "\n",
    "import sktime\n",
    "from sktime.datasets import load_from_tsfile\n",
    "\n",
    "x_train, y_train = load_from_tsfile(\"data/Ford_A/FordA_TRAIN.ts\")\n",
    "x_test, y_test = load_from_tsfile(\"data/Ford_A/FordA_TEST.ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-1,  1]), array([1846, 1755]))\n",
      "(array([-1,  1]), array([681, 639]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((3601, 500), (1320, 500), (3601,), (1320,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def readucr(filename):\n",
    "    data = np.loadtxt(filename, delimiter=\"\\t\")\n",
    "    y = data[:, 0]\n",
    "    x = data[:, 1:]\n",
    "    return x, y.astype(int)\n",
    "\n",
    "\n",
    "root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n",
    "\n",
    "x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n",
    "x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")\n",
    "\n",
    "print(np.unique(y_train, return_counts=True))\n",
    "print(np.unique(y_test, return_counts=True))\n",
    "\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_X(X):\n",
    "    np_data = []\n",
    "    lens = []\n",
    "    for i in range(len(X)):\n",
    "        line = X.iloc[i, 0]\n",
    "        lens.append(len(line))\n",
    "        np_data.append(line)\n",
    "        \n",
    "    #print(np.mean(lens), np.std(lens))\n",
    "    return np.array(np_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36010, 50) (13200, 50) (36010, 1) (13200, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1d8e47d850>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "window = 50\n",
    "len_seq = x_train.shape[1]\n",
    "n_patches = len_seq//window\n",
    "\n",
    "X_train = np.vstack([x_train[:, i:i+window] for i in range(n_patches)])\n",
    "X_test = np.vstack([x_test[:, i:i+window] for i in range(n_patches)])\n",
    "\n",
    "y_train = np.array([(int(y)+1) // 2 for y in y_train])\n",
    "y_test = np.array([(int(y)+1) // 2 for y in y_test])\n",
    "\n",
    "y_train = np.vstack([y_train.reshape(-1, 1) for i in range(n_patches)])\n",
    "y_test = np.vstack([y_test.reshape(-1, 1) for i in range(n_patches)])\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "plt.plot(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor =torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.int32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y, window=50):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.window=window\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "        \n",
    "#         start_ind = np.random.randint(0, len(X) - self.window)\n",
    "#         X = X[start_ind:start_ind+self.window]\n",
    "        X = X.reshape([-1, 1])\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "        return X, y\n",
    "\n",
    "    \n",
    "BS = 64    \n",
    "train_loader = DataLoader(MyDataset(X_train, y_train), batch_size=BS, shuffle=True)\n",
    "test_loader = DataLoader(MyDataset(X_test, y_test), batch_size=BS, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_net(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_layers, output_dim=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(1, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, data):\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(data)\n",
    "        hidden = hidden.reshape(hidden.shape[1], hidden.shape[2])\n",
    "        \n",
    "        hidden = self.dropout(hidden)\n",
    "        output = self.relu(self.fc1(hidden))\n",
    "        output = self.fc2(self.dropout(output))\n",
    "        output = torch.sigmoid(output)\n",
    "            \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, loader, criterion, device, optimizer, scheduler=None):\n",
    "    losses, n_batches = 0, 0\n",
    "    model.train(True)\n",
    "    for x, labels in loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x = x.to(device)\n",
    "        labels = labels.reshape(-1, 1).to(device)\n",
    "        \n",
    "        y_out = model(x)\n",
    "        loss = criterion(y_out, labels) \n",
    "        \n",
    "        loss.backward()     \n",
    "        optimizer.step()\n",
    "        losses += loss\n",
    "        n_batches += 1\n",
    "\n",
    "    mean_loss = losses / n_batches\n",
    "\n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "    \n",
    "    return mean_loss\n",
    "\n",
    "def valid_step(model, loader, criterion, device):\n",
    "    \n",
    "    losses, n_batches = 0, 0\n",
    "    model.eval()    \n",
    "    for x, labels in loader:\n",
    "        with torch.no_grad():\n",
    "            x = x.to(device)\n",
    "            labels = labels.reshape(-1, 1).to(device)\n",
    "\n",
    "            y_out = model(x)\n",
    "            loss = criterion(y_out, labels)\n",
    "            losses += loss\n",
    "\n",
    "            n_batches += 1\n",
    "\n",
    "    mean_loss = losses / n_batches\n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    pr = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return acc, pr, rec, f1\n",
    "    \n",
    "def estimate_epoch(loader, model=None, device='cpu', round_=True, multiclass=False):\n",
    "    \n",
    "    y_all_pred = torch.tensor([])\n",
    "    y_all_true = torch.tensor([])\n",
    "    \n",
    "    for X, y_true in loader:\n",
    "        X = X.to(device)\n",
    "        y_pred = model(X)\n",
    "        \n",
    "        if multiclass:\n",
    "            y_pred = torch.argmax(y_pred, axis=1)\n",
    "        else:\n",
    "            y_pred = torch.round(y_pred)\n",
    "        \n",
    "        y_all_true = torch.cat((y_all_true, y_true.cpu().detach()), dim=0)\n",
    "        y_all_pred = torch.cat((y_all_pred, y_pred.cpu().detach()), dim=0)\n",
    "        \n",
    "    y_all_pred = y_all_pred.numpy().reshape([-1, 1])\n",
    "    y_all_true = y_all_true.numpy().reshape([-1, 1])\n",
    "    \n",
    "    acc, pr, rec, f1 = calculate_metrics(y_all_true, y_all_pred)\n",
    "    \n",
    "    return acc, pr, rec, f1\n",
    "\n",
    "def train_procedure(model, train_loader, test_loader, criterion, optimizer, scheduler=None,\n",
    "                   num_epochs=30, step_print=5):\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_loss = train_step(model, train_loader, criterion, device, optimizer, scheduler)\n",
    "        test_loss = valid_step(model, test_loader, criterion, device) \n",
    "\n",
    "        acc_train, pr_train, rec_train, f1_train = estimate_epoch(train_loader, model, device=device)\n",
    "        acc_test, pr_test, rec_test, f1_test = estimate_epoch(test_loader, model, device=device)\n",
    "\n",
    "        if epoch % step_print == 0:\n",
    "            print(f'[Epoch {epoch + 1}] train loss: {train_loss:.3f}; acc_train {acc_train:.3f}; f1_train {f1_train:.3f}; test loss: {test_loss:.3f}; acc_test {acc_test:.3f}; f1_test {f1_test:.3f};')\n",
    "            plt.hist(y_test_pred)\n",
    "            plt.show()\n",
    "            \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor =torch.tensor(X_test, dtype=torch.float32)\n",
    "X_train_tensor = X_train_tensor.reshape([X_train_tensor.shape[0],X_train_tensor.shape[1], 1])\n",
    "X_test_tensor = X_test_tensor.reshape([X_test_tensor.shape[0],X_test_tensor.shape[1], 1])\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.int32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    pr = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return acc, pr, rec, f1\n",
    "\n",
    "def plotting(y_true, y_pred, window=1000):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(y_true[-window:], label = 'True')\n",
    "    plt.plot(y_pred[-window:], label = 'Pred')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1e3eabe6b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = 0\n",
    "torch.manual_seed(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTM_net(\n",
       "  (rnn): LSTM(1, 50, batch_first=True, dropout=0.3)\n",
       "  (fc1): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device= torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "\n",
    "num_epochs = 30\n",
    "print(num_epochs)\n",
    "LR = 0.001\n",
    "\n",
    "HIDDEN_DIM = 50\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 1\n",
    "DROPOUT = 0.3\n",
    "\n",
    "model = LSTM_net(HIDDEN_DIM, N_LAYERS, OUTPUT_DIM, DROPOUT).to(device)\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 12, gamma=0.1)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = train_procedure(model, train_loader, test_loader, criterion, optimizer,\n",
    "#                 num_epochs=15, step_print=5)\n",
    "\n",
    "#torch.save(model.state_dict(), f'checkpoints/Ford_A/model_{RS}_{col}.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device= torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "# #device = 'cpu'\n",
    "\n",
    "# num_epochs = 15\n",
    "# #print(num_epochs)\n",
    "# LR = 0.001\n",
    "\n",
    "# HIDDEN_DIM = 50\n",
    "# OUTPUT_DIM = 1\n",
    "# N_LAYERS = 1\n",
    "# DROPOUT = 0.3\n",
    "\n",
    "# for model_id in range(5):\n",
    "#     print(model_id)\n",
    "#     torch.manual_seed(model_id)\n",
    "\n",
    "#     model = LSTM_net(HIDDEN_DIM, N_LAYERS, OUTPUT_DIM, DROPOUT).to(device)\n",
    "\n",
    "#     criterion = torch.nn.BCELoss()\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "#     scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 12, gamma=0.1)\n",
    "    \n",
    "#     model = train_procedure(model, train_loader, test_loader, criterion, optimizer,\n",
    "#                 num_epochs=15, step_print=5)\n",
    "    \n",
    "#     torch.save(model.state_dict(), f'checkpoints/Ford_A/model_{model_id}_{col}.pth') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in tqdm(range(num_epochs)):\n",
    "\n",
    "#     train_loss = train_step(train_loader, model)\n",
    "#     test_loss = valid_step(test_loader, model) \n",
    "    \n",
    "#     X_test_tensor_loc = X_test_tensor #.reshape([X_test_tensor.shape[0], 1, X_test_tensor.shape[1]])\n",
    "#     X_train_tensor_loc = X_train_tensor #.reshape([X_train_tensor.shape[0], 1, X_train_tensor.shape[1]])\n",
    "    \n",
    "#     y_test_pred = all_predict_training(X_test_tensor_loc.to(device), model).astype('int').reshape(-1)\n",
    "#     y_train_pred = all_predict_training(X_train_tensor_loc.to(device), model).astype('int').reshape(-1)\n",
    "       \n",
    "#     acc_train, pr_train, rec_train, f1_train = calculate_metrics(y_train, y_train_pred)\n",
    "#     acc_test, pr_test, rec_test, f1_test = calculate_metrics(y_test, y_test_pred)\n",
    "    \n",
    "#     if epoch % 5 == 0:\n",
    "#         #print(f'[Epoch {epoch + 1}] train loss: {train_loss:.3f}; test loss: {test_loss:.3f};')\n",
    "#         print(f'[Epoch {epoch + 1}] train loss: {train_loss:.3f}; acc_train {acc_train:.3f}; f1_train {f1_train:.3f}; test loss: {test_loss:.3f}; acc_test {acc_test:.3f}; f1_test {f1_test:.3f};')\n",
    "#         plt.hist(y_test_pred)\n",
    "#         plt.show()\n",
    "        \n",
    "#     torch.save(model.state_dict(), f'checkpoints/Ford_A/model_{RS}_{col}.pth')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.estimation import (plot_aa_metrics, rejection_curves_procedure, \n",
    "                              build_basic_dict_curve, build_custom_dict_curve,\n",
    "                             draw_rejection_curves)\n",
    "from utils.attacks import ifgsm_procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_net(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_layers, output_dim=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(1, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, data):\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(data)\n",
    "        hidden = hidden.reshape(hidden.shape[1], hidden.shape[2])\n",
    "        \n",
    "        hidden = self.dropout(hidden)\n",
    "        output = self.relu(self.fc1(hidden))\n",
    "        output = self.fc2(self.dropout(output))\n",
    "        output = torch.sigmoid(output)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb75509bab434555a4630ae0ef64d5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************  EPS=0.001  ****************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321b5b3b006849f8b42d922321f45492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "fgsm_reg_attack() missing 1 required positional argument: 'alpha'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-60bf5dc15590>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     aa_res_df, rej_curves_dict = ifgsm_procedure(model=model, loader=test_loader, criterion=criterion, eps_params=eps_params,\n\u001b[0m\u001b[1;32m     24\u001b[0m                                                  n_steps=n_iters, n_objects=n_objects, train_mode=train_mode)\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AA/Package/utils/attacks.py\u001b[0m in \u001b[0;36mifgsm_procedure\u001b[0;34m(model, loader, criterion, eps_params, n_steps, metric_func, n_objects, train_mode)\u001b[0m\n\u001b[1;32m    222\u001b[0m         ifgsm_attack = IterGradAttack(model, loader, attack_func, attack_params,\n\u001b[1;32m    223\u001b[0m                                       criterion, n_steps, train_mode=train_mode)\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0maa_res_iter_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrej_curves_iter_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mifgsm_attack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_iterations_logging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulticlass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mrej_curves_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrej_curves_iter_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AA/Package/utils/attacks.py\u001b[0m in \u001b[0;36mrun_iterations_logging\u001b[0;34m(self, metric_fun, n_objects, multiclass)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_broken_objs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_iterations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maa_res_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrejection_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AA/Package/utils/attacks.py\u001b[0m in \u001b[0;36mrun_iterations\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 \u001b[0mx_adv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_one_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_one_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_adv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AA/Package/utils/attacks.py\u001b[0m in \u001b[0;36mrun_one_iter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;31m# attack for adv input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mx_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mx_adv_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_adv_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_adv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: fgsm_reg_attack() missing 1 required positional argument: 'alpha'"
     ]
    }
   ],
   "source": [
    "n_iters = 50\n",
    "eps_params = (1e-3, 1e0, 5)\n",
    "criterion = torch.nn.BCELoss()\n",
    "batch_size = BS\n",
    "n_objects = y_test.shape[0]\n",
    "\n",
    "HIDDEN_DIM = 50\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 1\n",
    "DROPOUT = 0.0\n",
    "\n",
    "device= torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "train_mode=True\n",
    "\n",
    "for model_id in range(1):\n",
    "    \n",
    "    path_to_saves = 'checkpoints/Ford_A/'\n",
    "\n",
    "    model = LSTM_net(HIDDEN_DIM, N_LAYERS, OUTPUT_DIM, DROPOUT).to(device)\n",
    "    model_path = path_to_saves + f'model_{0}_{col}.pth'\n",
    "    model.load_state_dict(copy.deepcopy(torch.load(model_path)))\n",
    "\n",
    "    aa_res_df, rej_curves_dict = ifgsm_procedure(model=model, loader=test_loader, criterion=criterion, eps_params=eps_params,\n",
    "                                                 n_steps=n_iters, n_objects=n_objects, train_mode=train_mode)\n",
    "\n",
    "#     aa_res_df.to_csv(f'results/Ford_A/aa_res_Ford_A_{model_id}.csv')\n",
    "#     with open(f'results/Ford_A/rej_curves_dict_Ford_A_model_{model_id}.pickle', 'wb') as file:\n",
    "#         pickle.dump(rej_curves_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aa_res_df= pd.read_csv(f'results/Ford_A/aa_res_Ford_A_{0}.csv', index_col=0)\n",
    "# aa_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_aa_metrics(aa_res_df, method='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_aa_metrics(aa_res_df, method='metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_LSTM(model, model_path):\n",
    "    model.load_state_dict(copy.deepcopy(torch.load(model_path)))\n",
    "    return model\n",
    "\n",
    "\n",
    "device= torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "\n",
    "\n",
    "model_params = {\"hidden_dim\":50,\n",
    "                \"output_dim\":1,\n",
    "                \"n_layers\":1,\n",
    "                \"dropout\":0.0,\n",
    "                }\n",
    "model = LSTM_net(**model_params).to(device)\n",
    "\n",
    "\n",
    "basic_path = 'checkpoints/Ford_A/model_{}_{}.pth'\n",
    "load_path = [basic_path.format(i, col) for i in range(5)]\n",
    "\n",
    "\n",
    "preds, norms = rejection_curves_procedure(model, test_loader,\n",
    "                               criterion, load_LSTM, load_path, y_test, device,\n",
    "                               n_models=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 0\n",
    "\n",
    "with open(f'results/Ford_A/rej_curves_dict_Ford_A_model_{model_id}.pickle', 'rb') as file:\n",
    "    rej_curves_dict = pickle.load(file)\n",
    "\n",
    "\n",
    "rejection_rates = np.linspace(0, 1.0, 20)\n",
    "iter_to_break = 49\n",
    "dict_metrics = {'Acc':accuracy_score,\n",
    "               #'ROC AUC':roc_auc_score,\n",
    "               #'PR AUC':average_precision_score\n",
    "               }\n",
    "\n",
    "all_eps = list(rej_curves_dict.keys())\n",
    "\n",
    "\n",
    "dict_curves = build_basic_dict_curve(y_test, preds, norms, rejection_rates, dict_metrics)\n",
    "dict_curves = build_custom_dict_curve(dict_curves, y_test, preds, norms, rejection_rates, \n",
    "                                     dict_metrics, rej_curves_dict, all_eps, iter_to_break)\n",
    "\n",
    "draw_rejection_curves(dict_curves, rejection_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ROC_RC = pd.DataFrame(columns=['metric', 'method', 'value'])\n",
    "\n",
    "for metric_name, dict_curve_metric in dict_curves.items():\n",
    "    \n",
    "    for method_name, list_val in dict_curve_metric.items():\n",
    "        val = np.array(list_val)\n",
    "        val_start = val[:-1]\n",
    "        val_post = val[1:]\n",
    "\n",
    "        val_mean = np.sum(np.array([val_start, val_post]))/(2*(len(rejection_rates)-1))\n",
    "        \n",
    "        dict_res = dict(zip(df_ROC_RC.columns,[metric_name, method_name, val_mean]))\n",
    "        df_ROC_RC = df_ROC_RC.append(dict_res, ignore_index=True)\n",
    "        \n",
    "df_ROC_RC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Получение адверсальных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_grad(model: nn.Module, state: bool = True) -> None:\n",
    "    \"\"\"Set requires_grad of all model parameters to the desired value.\n",
    "\n",
    "    :param model: the model\n",
    "    :param state: desired value for requires_grad\n",
    "    \"\"\"\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad_(state)\n",
    "\n",
    "def prepare_adv_data(\n",
    "        model: nn.Module,  # model for attack\n",
    "        loader: DataLoader,  # dataloader with data\n",
    "        criterion: nn.Module,\n",
    "        eps: float,\n",
    "        device='cpu',\n",
    "        train_mode=False):  # params_dict with eps and iter number\n",
    "\n",
    "    model.train(train_mode)\n",
    "    req_grad(model, state=False)  # detach all model's parameters\n",
    "\n",
    "    all_y_true = torch.tensor([]) # logging y_true for rebuilding dataloader and calculation difference with preds\n",
    "    x_adv_tensor = torch.FloatTensor([])  # logging x_adv for rebuilding dataloader\n",
    "\n",
    "    for x, y_true in loader:\n",
    "        \n",
    "        all_y_true = torch.cat((all_y_true, y_true.cpu().detach()), dim=0)\n",
    "        x.grad = None\n",
    "        x.requires_grad = True\n",
    "\n",
    "        # prediction for original input\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y_true = y_true.to(device)\n",
    "\n",
    "        y_pred = model(x)\n",
    "\n",
    "        # attack for adv input\n",
    "        loss_val = criterion(y_pred, y_true.reshape(-1, 1))\n",
    "        grad_ = torch.autograd.grad(loss_val, x, retain_graph=True)[0]\n",
    "        x_adv = x.data + eps * torch.sign(grad_)\n",
    "        x_adv_tensor = torch.cat((x_adv_tensor, x_adv.cpu().detach()), dim=0)\n",
    "\n",
    "    return x_adv_tensor.detach(), all_y_true.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor.shape, X_test_tensor.shape, y_train_tensor.shape, y_test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_adv.shape, y_adv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1\n",
    "criterion = torch.nn.BCELoss()\n",
    "batch_size = 256\n",
    "n_objects = y_train.shape[0]\n",
    "device= torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "train_mode=True\n",
    "\n",
    "HIDDEN_DIM = 50\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 1\n",
    "DROPOUT = 0.0\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "for model_id in tqdm(range(1, 5)):\n",
    "    \n",
    "    path_to_saves = 'checkpoints/Ford_A/'\n",
    "\n",
    "    model = LSTM_net(HIDDEN_DIM, N_LAYERS, OUTPUT_DIM, DROPOUT).to(device)\n",
    "    model_path = path_to_saves + f'model_{model_id}_{col}.pth'\n",
    "    model.load_state_dict(copy.deepcopy(torch.load(model_path)))\n",
    "    \n",
    "    # attack data    \n",
    "    X_adv, y_adv = prepare_adv_data(model, train_loader, criterion, eps, device, train_mode=train_mode)\n",
    "            \n",
    "    #rebuild dataset and dataloader\n",
    "    new_x_train, new_y_train = torch.concat([X_train_tensor, X_adv], dim=0), torch.concat([y_train_tensor, y_adv], dim=0)\n",
    "    train_adv_loader = DataLoader(MyDataset(new_x_train, new_y_train), batch_size=BS, shuffle=True)\n",
    "\n",
    "    \n",
    "    #train model on attacked data\n",
    "    torch.manual_seed(model_id)\n",
    "    model = LSTM_net(HIDDEN_DIM, N_LAYERS, OUTPUT_DIM, DROPOUT).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 12, gamma=0.1)\n",
    "\n",
    "    model = train_procedure(model, train_loader, test_loader, criterion, optimizer,\n",
    "                num_epochs=15, step_print=3)\n",
    "    #save model\n",
    "    torch.save(model.state_dict(), f'checkpoints/Ford_A/model_{model_id}_{col}_adv.pth') \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.TS2Vec import datautils\n",
    "\n",
    "datautils.load_UCR('Coffee')[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torch import nn\n",
    "\n",
    "# from typing import Dict, Any, Tuple, List, Union, Sequence, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_metrics_class(y_true, y_pred):\n",
    "#     acc = accuracy_score(y_true, y_pred)\n",
    "#     roc = roc_auc_score(y_true, y_pred)\n",
    "#     pr = average_precision_score(y_true, y_pred)\n",
    "#     return acc, roc, pr\n",
    "\n",
    "# def calc_accuracy(y_true, y_pred, y_pred_adv):\n",
    "#     acc_val = np.mean((y_pred == y_true))\n",
    "#     acc_adv = np.mean((y_pred_adv == y_true)) \n",
    "#     return acc_val, acc_adv\n",
    "\n",
    "# def req_grad(model: nn.Module, state: bool = True) -> None:\n",
    "#     \"\"\"Set requires_grad of all model parameters to the desired value.\n",
    "\n",
    "#     :param model: the model\n",
    "#     :param state: desired value for requires_grad\n",
    "#     \"\"\"\n",
    "#     for param in model.parameters():\n",
    "#         param.requires_grad_(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_df_aa_metrics(metric_dict, eps):\n",
    "    \n",
    "#     results_df = pd.DataFrame.from_dict(metric_dict, orient=\"index\")\n",
    "#     results_df.set_axis(\n",
    "#         pd.Index([\"ACC\", \"ROC AUC\", \"PR AUC\"], name=\"metric\"), axis=1, inplace=True\n",
    "#     )\n",
    "#     results_df.set_axis(\n",
    "#         pd.Index(results_df.index, name=\"n steps\",), axis=0, inplace=True,\n",
    "#     )\n",
    "\n",
    "#     results_df = results_df.reset_index()\n",
    "#     results_df['eps'] = eps\n",
    "#     return results_df\n",
    "\n",
    "# def ifgsm_one_iter(\n",
    "#     model: nn.Module, #model for attack\n",
    "#     loader: DataLoader, #dataloader with data\n",
    "#     criterion: nn.Module,\n",
    "#     eps: float,\n",
    "#     train_mode=False): #params_dict with eps and iter number\n",
    "\n",
    "#     model.train(train_mode)\n",
    "#     req_grad(model, state=False)  # detach all model's parameters\n",
    "\n",
    "#     all_y_true = [] #logging y_true for rebuilding dataloader and calculation difference with preds\n",
    "#     all_preds = [] #logging predictions original for calculation difference with data\n",
    "#     all_preds_adv = [] #logging predictions for calculation difference with data\n",
    "#     x_adv_tensor = torch.FloatTensor([]) #logging x_adv for rebuilding dataloader\n",
    "    \n",
    "\n",
    "#     for x, y_true in loader:\n",
    "\n",
    "#         all_y_true.extend(y_true.detach().data.numpy())\n",
    "        \n",
    "#         x.grad = None\n",
    "#         x.requires_grad = True\n",
    "        \n",
    "#         # prediction for original input\n",
    "#         x = x.to(device, non_blocking=True)\n",
    "#         y_true = y_true.to(device)\n",
    "        \n",
    "#         y_pred = model(x) \n",
    "           \n",
    "#         # attack for adv input\n",
    "#         loss_val = criterion(y_pred, y_true.reshape(-1, 1))        \n",
    "#         grad_ = torch.autograd.grad(loss_val, x, retain_graph=True)[0]\n",
    "#         x_adv = x.data + eps * torch.sign(grad_)\n",
    "#         x_adv_tensor = torch.cat((x_adv_tensor, x_adv.cpu().detach()), dim=0)\n",
    "        \n",
    "#         #assert (x_adv == x).sum() == 0, \"Data doesn't change after attack\"\n",
    "        \n",
    "#         #class prediction\n",
    "#         preds = torch.argmax(y_pred, dim=1).clone().cpu().detach()\n",
    "#         all_preds.extend(y_pred.cpu().detach().data.numpy())\n",
    "        \n",
    "#         # prediction for adv input\n",
    "#         with torch.no_grad():\n",
    "#             y_pred_adv = model(x_adv)\n",
    "#             preds_adv = torch.argmax(y_pred_adv, dim=1).clone().cpu().detach() #class prediction\n",
    "#             all_preds_adv.extend(y_pred_adv.cpu().detach().data.numpy())\n",
    "        \n",
    "#         #assert (y_pred_adv == y_pred).sum() == 0, \"Predicitions doesn't change after attack\"\n",
    "    \n",
    "#     return x_adv_tensor, all_y_true, all_preds, all_preds_adv\n",
    "    \n",
    "\n",
    "    \n",
    "# def ifgsm_iterations(model: nn.Module,\n",
    "#                      loader: DataLoader,\n",
    "#                      dataset: Dataset,\n",
    "#                      criterion: nn.Module,\n",
    "#                      eps: float,\n",
    "#                      n_steps: int,\n",
    "#                      metric_fun,\n",
    "#                      batch_size = 8,\n",
    "#                      n_objects=100,\n",
    "#                      multiclass=False,\n",
    "#                      train_mode=False,\n",
    "#                     ):\n",
    "    \n",
    "#     aa_res_dict = dict()    \n",
    "    \n",
    "#     rejection_dict = dict()\n",
    "#     rejection_dict['diff'] = dict()\n",
    "#     iter_broken_objs = np.array([10**7]*n_objects)\n",
    "    \n",
    "#     for iter_ in tqdm(range(n_steps)):\n",
    "        \n",
    "#         #attack\n",
    "#         x_adv_tensor, y_true, preds_original, preds_adv = ifgsm_one_iter(model, loader, criterion, \n",
    "#                                                                          eps, train_mode=train_mode) \n",
    "        \n",
    "#         if multiclass:\n",
    "#             preds_flat_round = np.argmax(np.array(preds_original), axis=1).flatten()\n",
    "#             preds_adv_flat_round = np.argmax(np.array(preds_adv), axis=1).flatten()\n",
    "#             y_true_flat =  np.array(y_true).flatten()\n",
    "#             shape_diff = (1, 2)\n",
    "            \n",
    "#         else:\n",
    "#             preds_flat_round = np.round_(np.array(preds_original)).flatten()\n",
    "#             preds_adv_flat_round = np.round_(np.array(preds_adv)).flatten()\n",
    "#             y_true_flat =  np.array(y_true).flatten()\n",
    "#             shape_diff = (1)\n",
    "            \n",
    "#         #estimation        \n",
    "#         if iter_ == 0:\n",
    "#             iter_broken_objs[preds_flat_round != y_true_flat] = iter_\n",
    "#             aa_res_dict[iter_] = metric_fun(y_true_flat, preds_flat_round)\n",
    "#             preds_iter_1 = np.array(preds_original)         \n",
    "            \n",
    "#         iter_broken_objs[(preds_adv_flat_round != y_true_flat) & (iter_broken_objs > iter_)] = iter_+1\n",
    "        \n",
    "#         rejection_dict['diff'][iter_+1] = np.sum((preds_iter_1 - np.array(preds_adv))**2, axis=shape_diff)\n",
    "        \n",
    "        \n",
    "#         rejection_dict['iter_broke'] = iter_broken_objs\n",
    "#         aa_res_dict[iter_+1] = metric_fun(y_true_flat, preds_flat_round)\n",
    "\n",
    "#         # rebuilding dataloader for new iteration\n",
    "#         it_dataset = dataset(x_adv_tensor, torch.tensor(y_true)) \n",
    "#         loader=DataLoader(it_dataset, batch_size=batch_size)\n",
    "        \n",
    "#     return aa_res_dict, rejection_dict\n",
    "\n",
    "# def ifgsm_procedure(model: nn.Module,\n",
    "#                     loader: DataLoader,\n",
    "#                     dataset: Dataset,\n",
    "#                     criterion: nn.Module,\n",
    "#                     eps_params: Tuple[float, float, int],\n",
    "#                     n_steps: int,\n",
    "#                     metric_fun,\n",
    "#                     batch_size = 8,\n",
    "#                     n_objects=100,\n",
    "#                     train_mode=False,\n",
    "#                    ):\n",
    "#     aa_res_df = pd.DataFrame()  \n",
    "    \n",
    "#     rej_curves_dict = dict() # multilevel dict  eps -> diff and object \n",
    "#     # diff -> #n_iteration -> np.array difference between original prediction without attack and broken predictions\n",
    "#     # object -> np.array n_iter when wrong prediction\n",
    "    \n",
    "#     eps_for_check = np.geomspace(*eps_params)\n",
    "    \n",
    "#     for eps in tqdm(eps_for_check):\n",
    "#         print(f'*****************  EPS={eps}  ****************')\n",
    "\n",
    "#         aa_res_iter_dict, rej_curves_iter_dict = ifgsm_iterations(model, loader, dataset, criterion, eps,  \n",
    "#                                                                   n_steps, metric_fun, batch_size, n_objects, \n",
    "#                                                                   train_mode=train_mode)\n",
    "\n",
    "#         rej_curves_dict[eps] = rej_curves_iter_dict\n",
    "#         aa_res_df = pd.concat([aa_res_df, build_df_aa_metrics(aa_res_iter_dict, eps)])\n",
    "        \n",
    "#     return aa_res_df, rej_curves_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_iters = 50\n",
    "# #eps_params = (1e-2, 1e0, 10)\n",
    "# eps_params = (0.3, 1e0, 1)\n",
    "# eps_params = (1e-3, 1e0, 5)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# aa_res_df, rej_curves_dict = ifgsm_procedure(model=model, dataloader=test_loader, criterion=criterion, \n",
    "#                                              eps_params=eps_params, n_steps=n_iters, \n",
    "#                                              metric_fun=calculate_metrics_class, batch_size=batch_size)\n",
    "\n",
    "# aa_res_df.to_csv(f'results/aa_res_{col}.csv')\n",
    "# with open(f'results/rej_curves_dict_{col}.pickle', 'wb') as file:\n",
    "#     pickle.dump(result_broket_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation AA res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_aa_metrics(res_df, method='eps'):\n",
    "    \n",
    "#     metrics = [\"ACC\", \"ROC AUC\", \"PR AUC\"]    \n",
    "#     if method=='eps':\n",
    "#         plt.figure(figsize=(15, 12))\n",
    "\n",
    "#         for i, eps in enumerate(res_df['eps'].unique()):    \n",
    "#             plt.subplot(2, 3, i+1)\n",
    "            \n",
    "#             for metric in metrics:\n",
    "#                 df_loc = res_df[res_df['eps'] == eps].copy()\n",
    "#                 plt.plot(df_loc.index, df_loc[metric], label=metric)\n",
    "#             plt.title(f'Eps = {round(eps, 4)}')\n",
    "#             plt.xlabel('n iterations')\n",
    "#             plt.legend()\n",
    "#             plt.grid()\n",
    "\n",
    "#     elif method=='metric':\n",
    "#         plt.figure(figsize=(20, 15))\n",
    "        \n",
    "#         for i, metric in enumerate(metrics):\n",
    "#             plt.subplot(2, 3, i+1)\n",
    "            \n",
    "#             for eps in res_df['eps'].unique():\n",
    "#                 df_loc = res_df[res_df['eps'] == eps].copy()\n",
    "#                 plt.plot(df_loc.index, df_loc[metric],  label=f'Eps = {round(eps, 4)}')\n",
    "#             plt.title(metric)\n",
    "#             plt.xlabel('n iterations')\n",
    "#             plt.legend()\n",
    "#             plt.grid()\n",
    "        \n",
    "#     else:\n",
    "#         raise ValueError('metho should be eps or metric')\n",
    "        \n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rejection curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_ensemble_mi(preds:  np.ndarray) -> np.ndarray:\n",
    "#     \"\"\"Calculate entropy of a given 1d numpy array. Input values are clipped to [0, 1].\n",
    "\n",
    "#     :param p: numpy array of numerics\n",
    "#     :return: numpy array of entropy values\n",
    "#     \"\"\"\n",
    "#     _, pentropy = get_ensemble_predictive_entropy(preds)\n",
    "#     ave_preds, eentropy = get_ensemble_expected_entropy(preds)\n",
    "\n",
    "#     return ave_preds, (pentropy - eentropy)\n",
    "\n",
    "# def get_minprob(pred: np.ndarray) -> np.ndarray:\n",
    "#     \"\"\"Calculate minprob uncertainty estimate. Input values are clipped to [0, 1].\n",
    "#     :param pred: numpy array of numerics\n",
    "#     :return: numpy array of minprob estimates\n",
    "#     \"\"\"\n",
    "\n",
    "#     def func(pred):\n",
    "#         if pred < 0.5:\n",
    "#             return pred, pred\n",
    "#         else:\n",
    "#             return pred, (1 - pred)\n",
    "\n",
    "#     func_vec = np.vectorize(func)\n",
    "#     clipped_pred = np.clip(pred, 1e-5, 1 - 1e-5)\n",
    "\n",
    "#     return func_vec(clipped_pred)\n",
    "\n",
    "# def get_entropy(p: np.ndarray) -> np.ndarray:\n",
    "#     \"\"\"Calculate entropy of a given 1d numpy array. Input values are clipped to [0, 1].\n",
    "\n",
    "#     :param p: numpy array of numerics\n",
    "#     :return: numpy array of entropy values\n",
    "#     \"\"\"\n",
    "#     cp = np.clip(p, 1e-5, 1 - 1e-5)\n",
    "#     entropy = -cp * np.log2(cp) - (1 - cp) * np.log2(1 - cp)\n",
    "\n",
    "#     return entropy\n",
    "\n",
    "\n",
    "\n",
    "# def get_ensemble_predictive_entropy(preds: np.ndarray) -> np.ndarray:\n",
    "#     \"\"\"Calculate predictive entropy of ensemble predictions.\n",
    "#     :param preds: numpy array of ensemble predictions. Expects first dimension to represent members of ensemble\n",
    "#     :return: numpy array of predictive entropy estimates\n",
    "#     \"\"\"\n",
    "#     ave_preds = np.average(np.copy(preds), axis=0)\n",
    "\n",
    "#     return ave_preds, get_entropy(ave_preds)\n",
    "\n",
    "\n",
    "# def get_ensemble_expected_entropy(preds: np.ndarray) -> np.ndarray:\n",
    "#     \"\"\"Calculate expected entropy of ensemble predictions.\n",
    "#     :param preds: numpy array of ensemble predictions. Expects first dimension to represent members of ensemble\n",
    "#     :return: numpy array of predictive entropy estimates\n",
    "#     \"\"\"\n",
    "#     ave_preds = np.average(np.copy(preds), axis=0)\n",
    "\n",
    "#     return ave_preds, np.apply_along_axis(get_entropy, 0, preds).mean(axis=0)\n",
    "\n",
    "\n",
    "# def get_ensemble_std(preds: np.ndarray) -> np.ndarray:\n",
    "#     \"\"\"Calculate estimate of standard deviation of ensemble predictions.\n",
    "#     :param preds: numpy array of ensemble predictions. Expects first dimension to represent members of ensemble\n",
    "#     :return: numpy array of standard deviation estimates estimates\n",
    "#     \"\"\"\n",
    "#     ave_preds = np.average(np.copy(preds), axis=0)\n",
    "\n",
    "#     return ave_preds, np.std(preds, axis=0)\n",
    "\n",
    "\n",
    "# def sort_data_by_metric(\n",
    "#     metric: Sequence, preds: np.ndarray, labels: np.ndarray\n",
    "# ) -> Tuple[List, List]:\n",
    "#     \"\"\"Sort preds and labels by descending uncertainty metric.\n",
    "#     :param metric: uncertainty metric according to which preds and labels will be sorted\n",
    "#     :param preds: model predictions\n",
    "#     :param labels: ground truth labels\n",
    "#     :return: a tuple of\n",
    "#         - np.ndarray of predictions, sorted according to metric\n",
    "#         - np.ndarray of labels, sorted according to metric\n",
    "#     \"\"\"\n",
    "#     sorted_metric_idx = np.argsort(metric)\n",
    "\n",
    "#     return preds[sorted_metric_idx].flatten(), labels[sorted_metric_idx].flatten()\n",
    "\n",
    "\n",
    "# def get_upper_bound_idx(data_len: int, rejection_rates: Sequence[float]) -> List[float]:\n",
    "#     \"\"\"Calculate upped bounds on indices of data arrays.\n",
    "#     Based on corresponding list of rejection rates is applied.\n",
    "#     :param data_len: length of data array\n",
    "#     :param rejection_rates: array of rejection rates to calculate upper bounds for\n",
    "#     :return: list of upper bounds\n",
    "#     \"\"\"\n",
    "#     idx = []\n",
    "#     for rate in rejection_rates:\n",
    "#         idx.append(\n",
    "#             min(np.ceil(data_len * (1 - rate)), np.array(data_len)).astype(int).item()\n",
    "#         )\n",
    "\n",
    "#     return idx\n",
    "\n",
    "\n",
    "# def reject_and_eval(\n",
    "#     preds: np.ndarray,\n",
    "#     labels: np.ndarray,\n",
    "#     upper_bounds: Sequence[float],\n",
    "#     scoring_func: Callable,\n",
    "# ) -> List:\n",
    "#     \"\"\"Clip preds and labels arrays.\n",
    "#     Using list of upper bounds, and calculate scoring metric for\n",
    "#     predictions after rejection.\n",
    "#     :param preds: model label predictions or predicted class probabilities\n",
    "#     :param labels: ground truth labels\n",
    "#     :param upper_bounds: list of upper bounds to clip preds and labels to\n",
    "#     :param scoring_func: scoring function that takes labels and predictions or probabilities (in that order)\n",
    "#     :return: list of scores calculated for each upper bound\n",
    "#     \"\"\"\n",
    "#     scores = []\n",
    "#     predicted_labels = np.where(preds > 0.5, 1, 0)\n",
    "    \n",
    "    \n",
    "#     i = 0\n",
    "#     for upper_bound in upper_bounds:\n",
    "#         predicted_labels_below_thresh = predicted_labels[0:upper_bound]\n",
    "#         preds_below_thresh = preds[0:upper_bound]\n",
    "#         labels_below_thresh = labels[0:upper_bound]\n",
    "\n",
    "#         try:\n",
    "#             if preds_below_thresh.size > 0 and labels_below_thresh.mean() not in [0, 1]:\n",
    "#                 scores.append(scoring_func(labels_below_thresh, preds_below_thresh))\n",
    "                \n",
    "#         except ValueError:\n",
    "#             if (\n",
    "#                 predicted_labels_below_thresh.size > 0\n",
    "#                 and labels_below_thresh.mean() not in [0, 1]\n",
    "#             ):\n",
    "#                 scores.append(\n",
    "#                     scoring_func(labels_below_thresh, predicted_labels_below_thresh)\n",
    "#                 )\n",
    "#         i += 1\n",
    "    \n",
    "#     #print(scores)\n",
    "#     return scores\n",
    "\n",
    "\n",
    "# def reject_by_metric(\n",
    "#     get_metric: Callable,\n",
    "#     preds: np.ndarray,\n",
    "#     labels: np.ndarray,\n",
    "#     rejection_rates: List[float],\n",
    "#     scoring_func: Callable,\n",
    "# ) -> List:\n",
    "#     \"\"\"Reject points from preds and labels based on uncertainty estimate of choice.\n",
    "#     :param get_metric: function that returns uncertainty metric for given model predictions\n",
    "#     :param preds: model label predictions or predicted class probabilities\n",
    "#     :param labels: ground truth labels\n",
    "#     :param rejection_rates: list of rejection rates to use\n",
    "#     :param scoring_func: scoring function that takes labels and predictions or probabilities (in that order)\n",
    "#     :return: list of scores calculated for each upper bound\n",
    "#     \"\"\"\n",
    "    \n",
    "#     #print(scoring_func)\n",
    "\n",
    "#     preds, metric_values = get_metric(preds)\n",
    "    \n",
    "#     preds_sorted, labels_sorted = sort_data_by_metric(metric_values, preds, labels)\n",
    "#     #print(preds_sorted.shape, labels_sorted.shape)\n",
    "    \n",
    "#     upper_indices = get_upper_bound_idx(preds.size, rejection_rates)\n",
    "    \n",
    "#     res = reject_and_eval(preds_sorted, labels_sorted, upper_indices, scoring_func)\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reject_by_iters_obj(\n",
    "#     iters_vec: np.ndarray,\n",
    "#     preds: np.ndarray,\n",
    "#     labels: np.ndarray,\n",
    "#     rejection_rates: List[float],\n",
    "#     scoring_func: Callable,\n",
    "# ) -> List:\n",
    "    \n",
    "#     preds = np.average(np.copy(preds), axis=0)\n",
    "#     preds_sorted, labels_sorted = sort_data_by_metric(1/(iters_vec+0.001), preds, labels)\n",
    "#     #preds_sorted, labels_sorted = sort_data_by_metric(iters_vec, preds, labels)\n",
    "    \n",
    "#     upper_indices = get_upper_bound_idx(preds.size, rejection_rates)\n",
    "#     res = reject_and_eval(preds_sorted, labels_sorted, upper_indices, scoring_func)\n",
    "#     return res\n",
    "\n",
    "# def reject_by_diff(\n",
    "#     diff_vec: np.ndarray,\n",
    "#     preds: np.ndarray,\n",
    "#     labels: np.ndarray,\n",
    "#     rejection_rates: List[float],\n",
    "#     scoring_func: Callable,\n",
    "# ) -> List:\n",
    "    \n",
    "#     preds = np.average(np.copy(preds), axis=0)\n",
    "    \n",
    "#     preds_sorted, labels_sorted = sort_data_by_metric(1/(diff_vec+0.001), preds, labels)\n",
    "#     #preds_sorted, labels_sorted = sort_data_by_metric(diff_vec, preds, labels)\n",
    "    \n",
    "#     upper_indices = get_upper_bound_idx(preds.size, rejection_rates)\n",
    "#     res = reject_and_eval(preds_sorted, labels_sorted, upper_indices, scoring_func)\n",
    "#     return res\n",
    "\n",
    "# def reject_by_norm(\n",
    "#     norm_vec: np.ndarray,\n",
    "#     preds: np.ndarray,\n",
    "#     labels: np.ndarray,\n",
    "#     rejection_rates: List[float],\n",
    "#     scoring_func: Callable,\n",
    "# ) -> List:\n",
    "    \n",
    "#     preds = np.average(np.copy(preds), axis=0)\n",
    "\n",
    "#     #print(len(iters_vec), len(preds), len(labels))\n",
    "#     #preds_sorted, labels_sorted = sort_data_by_metric(1/(norm_vec+1), preds, labels)\n",
    "#     preds_sorted, labels_sorted = sort_data_by_metric(norm_vec, preds, labels)\n",
    "    \n",
    "#     upper_indices = get_upper_bound_idx(preds.size, rejection_rates)\n",
    "#     res = reject_and_eval(preds_sorted, labels_sorted, upper_indices, scoring_func)\n",
    "#     return res\n",
    "\n",
    "# def reject_randomly(\n",
    "#     preds: np.ndarray,\n",
    "#     labels: np.ndarray,\n",
    "#     rejection_rates: List[float],\n",
    "#     num_samples: int,\n",
    "#     scoring_func: Callable,\n",
    "# ) -> np.ndarray:\n",
    "#     \"\"\"Reject predictions after random shuffling.\n",
    "\n",
    "#     Perform sampling num_samples times for each rejection rate and average over them\n",
    "\n",
    "#     :param preds: model label predictions or predicted class probabilities\n",
    "#     :param labels: ground truth labels\n",
    "#     :param rejection_rates: list of rejection rates to use\n",
    "#     :param num_samples: number of repetitions of shuffling + rejection\n",
    "#     :param scoring_func: scoring function that takes labels and predictions or probabilities (in that order)\n",
    "#     :return: list of scores calculated for each upper bound\n",
    "#     \"\"\"\n",
    "#     accs = []\n",
    "#     upper_indices = get_upper_bound_idx(preds.size, rejection_rates)\n",
    "\n",
    "#     for _ in range(num_samples):\n",
    "#         shuffle_indices = np.random.permutation(preds.size)\n",
    "#         accs.append(\n",
    "#             reject_and_eval(\n",
    "#                 preds[shuffle_indices],\n",
    "#                 labels[shuffle_indices],\n",
    "#                 upper_indices,\n",
    "#                 scoring_func,\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#     return np.mean(accs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_basic_dict_curve(\n",
    "#     labels, \n",
    "#     preds:list,\n",
    "#     norms:np.array,\n",
    "#     rejection_rates:np.array, \n",
    "#     dict_metrics:dict,\n",
    "#     ):\n",
    "\n",
    "#     dict_curves = dict()\n",
    "    \n",
    "#     preds = np.array(preds)\n",
    "#     ave_preds = np.average(preds, axis=0)\n",
    "\n",
    "#     for metric_name, metric in  dict_metrics.items():\n",
    "        \n",
    "\n",
    "#         dict_curve_metric = dict()\n",
    "\n",
    "\n",
    "#         dict_curve_metric['Predictive entropy'] = reject_by_metric(get_ensemble_predictive_entropy,\n",
    "#                                          preds,\n",
    "#                                          labels, \n",
    "#                                          rejection_rates,\n",
    "#                                          metric)\n",
    "\n",
    "\n",
    "#         dict_curve_metric['StD'] = reject_by_metric(get_ensemble_std,\n",
    "#                                     preds,\n",
    "#                                     labels, \n",
    "#                                     rejection_rates,\n",
    "#                                     metric)\n",
    "\n",
    "#         dict_curve_metric['MaxProb'] = reject_by_metric(get_minprob,\n",
    "#                                         preds[0, :],\n",
    "#                                         labels, \n",
    "#                                         rejection_rates,\n",
    "#                                         metric)\n",
    "\n",
    "#         dict_curve_metric['Random'] = reject_randomly(ave_preds,\n",
    "#                                       labels, \n",
    "#                                       rejection_rates,\n",
    "#                                       1000,\n",
    "#                                       metric)\n",
    "\n",
    "#         dict_curve_metric['Grad_Norm'] = reject_by_norm(norms,\n",
    "#                                     preds,\n",
    "#                                     labels, \n",
    "#                                     rejection_rates,\n",
    "#                                     metric)\n",
    "        \n",
    "        \n",
    "#         dict_curves[metric_name] = dict_curve_metric\n",
    "#     return dict_curves\n",
    "\n",
    "\n",
    "# def build_custom_dict_curve(dict_curves:dict,\n",
    "#                         labels, \n",
    "#                         preds:list,\n",
    "#                         norms:np.array,\n",
    "#                         rejection_rates:np.array, \n",
    "#                         dict_metrics:dict,\n",
    "#                         rej_curves_dict:dict, \n",
    "#                         all_eps:list,\n",
    "#                         iter_to_break:int, ):\n",
    "\n",
    "#     for metric_name, metric in dict_metrics.items():\n",
    "    \n",
    "\n",
    "#         dict_curve_metric = dict()\n",
    "        \n",
    "#         type_ = 'diff'\n",
    "#         eps = all_eps[0]\n",
    "#         iter_ = iter_to_break\n",
    "#         rej_vec = abs(rej_curves_dict[eps][type_][iter_])\n",
    "        \n",
    "#         dict_curve_metric[f'Iter_{type_}_eps={round(eps, 4)}_iter={iter_}'] = reject_by_diff(rej_vec,\n",
    "#                                          preds,\n",
    "#                                          labels, \n",
    "#                                          rejection_rates,\n",
    "#                                          metric)\n",
    "\n",
    "\n",
    "#         type_ = 'diff'\n",
    "#         eps = all_eps[2]\n",
    "#         iter_ = iter_to_break\n",
    "#         rej_vec = abs(rej_curves_dict[eps][type_][iter_])\n",
    "#         dict_curve_metric[f'Iter_{type_}_eps={round(eps, 4)}_iter={iter_}'] = reject_by_diff(rej_vec,\n",
    "#                                          preds,\n",
    "#                                          labels, \n",
    "#                                          rejection_rates,\n",
    "#                                          metric)\n",
    "\n",
    "\n",
    "#         type_ = 'iter_broke'\n",
    "#         eps = all_eps[4]\n",
    "#         iter_ = ''\n",
    "#         dict_curve_metric[f'Iter_{type_}_eps={round(eps, 4)}'] = reject_by_iters_obj(rej_curves_dict[eps][type_],\n",
    "#                                          preds,\n",
    "#                                          labels, \n",
    "#                                          rejection_rates,\n",
    "#                                          metric)\n",
    "        \n",
    "#         # Union 2 dicts\n",
    "#         dict_curves[metric_name] = {**dict_curves[metric_name], **dict_curve_metric}\n",
    "\n",
    "        \n",
    "#     return dict_curves\n",
    "    \n",
    "# def draw_rejection_curves(dict_curves):\n",
    "#     plt.figure(figsize=(17, 9))\n",
    "\n",
    "#     for i, (metric_name, curves_dict) in enumerate(dict_curves.items()):\n",
    "\n",
    "#         plt.subplot(1, 3, i+1)\n",
    "\n",
    "#         for label, metric in curves_dict.items():\n",
    "#             if 'Iter_diff' in label:\n",
    "#                 plt.plot(rejection_rates[0:len(metric)], metric, \n",
    "#                          label=label, linewidth = 3, linestyle='--')\n",
    "#             elif 'Iter_object' in label:\n",
    "#                 plt.plot(rejection_rates[0:len(metric)], metric, \n",
    "#                          label=label, linewidth = 3, linestyle='-')\n",
    "\n",
    "#             elif 'Norm' in label:\n",
    "#                 plt.plot(rejection_rates[0:len(metric)], metric, \n",
    "#                          label=label, linewidth = 3, linestyle=':')\n",
    "#             else:\n",
    "#                 plt.plot(rejection_rates[0:len(metric)], metric, label=label)\n",
    "#         plt.title(metric_name)\n",
    "#         plt.xlabel('Rejection rate')\n",
    "#         plt.ylabel(metric_name)\n",
    "#         plt.legend()\n",
    "#         plt.grid()\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "\n",
    "# def all_predict(model, loader, round_=True, multiclass=False, device='cpu'):\n",
    "    \n",
    "#     y_all_pred = torch.FloatTensor([])\n",
    "    \n",
    "#     for x, y_true in loader:\n",
    "            \n",
    "#         x = x.to(device)\n",
    "#         y_true = y_true.to(device)\n",
    "#         y_pred = model(x)\n",
    "        \n",
    "#         if round_: \n",
    "#             if multiclass:\n",
    "#                 y_pred = torch.argmax(y_pred, dim=1)\n",
    "#             else:\n",
    "#                 y_pred = torch.round_(y_pred)\n",
    "#         else:\n",
    "#             if multiclass:\n",
    "#                 y_pred = torch.nn.functional.softmax(y_pred)\n",
    "            \n",
    "            \n",
    "#         y_all_pred = torch.cat((y_all_pred, y_pred.cpu().detach()), dim=0)\n",
    "        \n",
    "#     if round_:\n",
    "#         y_all_pred = torch.round(y_all_pred)\n",
    "#     y_all_pred = y_all_pred.cpu().detach().numpy()\n",
    "#     y_all_pred = y_all_pred.reshape([-1, 1])\n",
    "#     return y_all_pred\n",
    "\n",
    "# def get_grad_norm(model:nn.Module,\n",
    "#                   loader:DataLoader,\n",
    "#                   criterion:nn.Module,\n",
    "#                   ):\n",
    "#     grad_norm_model = torch.tensor(np.array([]))\n",
    "    \n",
    "\n",
    "#     for x, y_true in loader:\n",
    "        \n",
    "#         x.grad = None\n",
    "#         x.requires_grad = True\n",
    "        \n",
    "#         x = x.to(device)\n",
    "#         y_true = y_true.to(device)\n",
    "\n",
    "#         y_pred = model(x)\n",
    "#         loss_val = criterion(y_pred, y_true)\n",
    "\n",
    "#         grad = torch.autograd.grad(loss_val, x, retain_graph=True)[0]\n",
    "#         grad_norm = torch.linalg.norm(grad, ord=2, dim=(1,2)).cpu().detach()\n",
    "#         grad_norm_model = torch.cat((grad_norm_model, grad_norm))\n",
    "        \n",
    "#     return np.array(grad_norm_model)\n",
    "\n",
    "# def rejection_curves_procedure(model:nn.Module, \n",
    "#                                loader:DataLoader,\n",
    "#                                criterion:nn.Module,\n",
    "#                                load_fun, \n",
    "#                                load_params,\n",
    "#                                labels,\n",
    "#                                device,\n",
    "#                                n_models=1,\n",
    "#                               ):\n",
    "    \n",
    "#     norms_all = []\n",
    "#     preds_all = []\n",
    "    \n",
    "#     for i, model_path in enumerate(load_path):\n",
    "\n",
    "#         #loading_weights\n",
    "        \n",
    "#         model = load_fun(model=model, model_path=model_path)\n",
    "#         model = model.to(device)\n",
    "#         model.eval()\n",
    "\n",
    "#         #estimate results        \n",
    "#         preds_round = all_predict(model=model, loader=loader, \n",
    "#                               round_=True, device=device)\n",
    "#         metrics = calculate_metrics_class(labels, preds_round)\n",
    "#         acc, roc, pr_auc = metrics\n",
    "#         print(f\"{i}th models Accuracy {acc:.3f}, ROC-AUC {roc:.3f}, PR-AUC {pr_auc:.3f}\")\n",
    "        \n",
    "#         preds = all_predict(model=model, loader=loader, \n",
    "#                               round_=False, device=device)\n",
    "#         preds_all.append(preds)\n",
    "        \n",
    "#         model.train()\n",
    "#         norm = get_grad_norm(model=model, loader=loader, criterion=criterion)\n",
    "#         norms_all.append(norm)\n",
    "        \n",
    "#     preds = np.array(preds_all).reshape(len(preds_all), preds.shape[0])\n",
    "#     return preds, np.sum(np.array(norms_all), axis=0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_params = {\"HIDDEN_DIM\":50,\n",
    "#                 \"OUTPUT_DIM\":1,\n",
    "#                 \"N_LAYERS\":1,\n",
    "#                 \"DROPOUT\":0.3,\n",
    "#                 }\n",
    "\n",
    "# basic_path = 'checkpoint/model_{}_{}.pth'\n",
    "# load_path = [basic_path.format(i, col) for i in range(5)]\n",
    "\n",
    "# model = LSTM_net(**model_params).to(device)\n",
    "# preds, norms = rejection_curves_procedure(model, test_loader,\n",
    "#                                criterion, load_path, \n",
    "#                                n_models=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_params = {\"HIDDEN_DIM\":50,\n",
    "#                 \"OUTPUT_DIM\":1,\n",
    "#                 \"N_LAYERS\":1,\n",
    "#                 \"DROPOUT\":0.3,\n",
    "#                 }\n",
    "\n",
    "# basic_path = 'checkpoint/model_{}_{}.pth'\n",
    "# load_path = [basic_path.format(i, col) for i in range(5)]\n",
    "# preds, norms = rejection_curves_procedure(model, loader,\n",
    "#                                criterion, load_path, \n",
    "#                                n_models=5, model_params)\n",
    "\n",
    "# rejection_rates = np.linspace(0, 1.0, 20)\n",
    "# iter_to_break = 49\n",
    "# dict_metrics = {'Acc':accuracy_score,\n",
    "#                'ROC AUC':roc_auc_score,\n",
    "#                'PR AUC':average_precision_score}\n",
    "\n",
    "# all_eps = list(result_broket_dict.keys())\n",
    "\n",
    "\n",
    "# dict_curves = build_basic_dict_curve(y_test, preds, norms, rejection_rates, dict_metrics)\n",
    "# dict_curves = build_basic_dict_curve(dict_curves, y_test, preds, norms, rejection_rates, \n",
    "#                                      dict_metrics, rej_curves_dict, all_eps, iter_to_break)\n",
    "\n",
    "\n",
    "# dict_curves[metric_name] = dict_curve_metric\n",
    "# draw_rejection_curves(dict_curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_rejection_curves(dict_curves)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
